so so the point of of these meetings is that everybody very loudly says and enters in the list actually in the in the google sheet, that is more important, the number of items that you have worked on and that you think you deserve some <unintelligible>.
so that's the that's the point of the meetings.
so please make sure that your point from the last week already have a row and then when they have the row i can end the i can end that.
one thing that i want you to say loud is that i've agreed, i was asked by [person13] to give her presentation on [project1] on the monday seminar so as from subtitling we are also expecting to describe of what we are doing regular talk there.
the date i think the date is possible the 17th february so in little bit more than two weeks from now.
so i need to work on this which is so so this is a good place for better to use english as english models because i think that this english models could be better adapted.
so the monday seminar on the 17th happens the week after we have some dry run of a workshop where we are describing various language technologies and that's for the [organization6] congress the part like the (site) activity.
and then the the important big event that i wanted to talk about is the students firm fair thats something which we have done only last year.
and it's very noisy environment it's like fair or congress in big hall of many stands and there is little a little side it's not really room, it's just a dedicated area where this competition of the firm presentations is is running.
and we are also going to (writers), well it will depend what the they will let us or not, last year we were we were showing some of the subtitles and because that was were appearing just once.
this is one of the to do or wanted items to add some filter and f- populated what words that we don't want to show at all.
so that's it is top so it's also the questions what to do of these topics then rather we should removes such words or not.
so as you remember on the the day before yesterday were [person3] giving (advering) talk one [location4] word which has nothing in relation that was translated as scrotum and so this is exactly the type of word which is bad for high school students.
so and i would like to ask everybody to to say what they did and think about what activity they would like to work on.
and because the from the ger- [other1] thing of the office i have downloaded 1970 to 1980 pdf files which is extracted which was extracted from the [other2] using texted [other3] so just can work in text format.
(person7) i don't know what he is using the tools what is the aplication so it would be nice when we would be -
(person2) so for the english or for both of them these are languages anyway so there the adaptation by [person6] make sense it's good to put this into the collection and start organizing the collection.
so if the idea is that the compress sound goes always to network and is directly decompress before (begin sent).
so let it circumstance we had that the that is not actually <unintelligible> that is not supposed to be a prophane what coming up.
(person2) so i have an idea but i don't know if we have the person for that and the idea is exactly responding to the to the fake that.
the list will never be complete and it will contain many word which we actually like would like to have an as the output or as the input.
so somehow there was lighter the [location4] word was "soupatko" <another language=""> which is a slider actually and for some reason in some of the corpora this must have been in the same sentence as as the error came from.
so the idea i have in mind is to train empty systems on corpora which are refind to contain only higher frequency words so like safe word corpora or safe vocabulary corpora.
so this is if we do this type of filtering firstly together a very huge corpus then we need to set up threshold like what was the safe we can work work frequent boundary and then we need to create the corpus which is somehow limited to these words.
and that is hard, somewhat, but i think that we could do it monolingualy and use back translation to create the other part.
and the third option is to use this filtering on in the monolingual setting only and use backtranslation to get the other side.
so that's a like a bigger experiment and i would probably find someone new for that one semestr stars from the students but it's it is an option.
(person16) so as you may know i finished evaluation framework and it's ready but maybe for some test maybe i should do some a small changes but i think it's ready.
we (approved) that time base and word base segmentation to find the for example where is the may i want to calculated delay i should estimate that time of each word should be the time expected time for each word.
so this is the team and now i me and [person8] working on the i and [person8] are working on the paper for (exceptement) and i think that's on.
so i think that it would still be interesting to run your forced alignment of this data and we you can now use the existing time stamps to break the long recording into shorter ones.
so i think that i was quite happy with result then during the presentation as like quite a lot of domain specific words were recognized by the model.
whenever you think it something big and i think this the improvement [person3] talk it's very good one so definitely deserve to be listed.
[person1] has found the appropriate command flacs so that the audio is compress to mp3 then shiped as mp3 to network and then decompress before being sent to the asr.
so if the if there is something we can lost of of  <unintelligible>  then we don't want to do it but if it's like the same then will write to <unintelligible> because it simply makes the communication more (abased) less likely to be to be effected by network long.
because i noticed when i read the paper about librispeech that the recordings were before in mp3 format and then they were actually converted into flac format and then i had to convert it into wav format so and the and the common voice is also in mp3.
now my systemlooks in the window with a greedy decoding and looks for pauses bef- between words and i have window that must be at least four seconds long and maximum is eight seconds found find the the most probable pause between the words and then i cut the windows there.
but the results were at least for at least for some [organization2] talks they were really bad because the the corpora on which the transform were trade was trained is for casual speak and and fairy tales and so on.
so that's actual translation with the with the transformer was quite bad and the transform-hallucinated some words.
and i i sent an e-mail to [person18] and i haven't received any any comments to to this new segment- to this new windowing from him.
so do you estimate that than your segmenter could be operational in the two weeks from now for the monday talk by 17th or even for the dry run sesion of the workshop on the 12th and you can talk.
and the next problem is that the segmentation should be done on speaker separately because when there is some conversation the windows can overlap to speakers and the problem is then then the transformer translate these sentences into nonsense.
(person2) so the talks in general are one speaker only but do it also as an for example for the remote calls there conference calls on the interview platform, there each speaker has different chanel so the diarization is there for free and it´s not not mixed.
(person15) that sould be better and the next problem is that the windows overlaped sentences and this causes some problems too.
and we need the (byplans) to be wilow cashing so we created an modify version and there was there was one of the early sessions.
(person10) yes, so so far i have only worked on the paraphrasing and actually i just w- i just wanted to say that right now i'm just waiting for a virtual machine for the paraphrasing server but otherwise it is done and working.
i remember that i have asked him to check if the virtual machine set up is is reasonable and something like that and then forward to the it department.
i think it should be simply and totaly independent process for now which anybody could run to adition to be sound acqusition pipeline and would that see which of the input chanell is receiving what output.
(person10) yeah, so i just try it today and it's just basicly sais that it´s like three hundred times faster for asr so so so it could i just told it could actually be used to do asr like on on the spot and it couldn't have to be sent and the delay problem could be eliminated.
(person10) and they also say it is that it is robust and dust nearly eliminates words keeping <unintelligible> which is also interesting interesting so i can just send it to you if you like.
