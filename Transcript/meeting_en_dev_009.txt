<another-language>

<other_noise>

(person2) we still didn't have [person1].

i'm not sure if he will connect

i expect him to connect.

so anyway let's get started.

thanks for coming.

again we are trying to record this call to see if we are able to summarize it automatically.

<parallel_talk>

document we are talking about -

so our probably -

so so the point of of these meetings is that everybody very loudly says and enters in the list actually in the in the google sheet, that is more important, the number of items that you have worked on and that you think you deserve some <unintelligible>.

so that's the that's the point of the meetings.

so that's list.

so please make sure that your point from the last week already have a row and then when they have the row i can end the i can end that.

so briefly describe all what you did what do you think deserve some <unintelligible> because it is usefull for at the all all of that.

so it is a practise in selling your homework.

and that that aplies also to [person14], [person15] and [person5] of course.

and everybody else who should also come later.

so that's yeah -

one thing that i want you to say loud is that i've agreed, i was asked by [person13] to give her presentation on [project1] on the monday seminar so as from subtitling we are also expecting to describe of what we are doing regular talk there.

the date i think the date is possible the 17th february so in little bit more than two weeks from now.

and the goal of office would be obviously to describe the pipeline say where there problems and to invite more people to join us.

so that's why i pick this (every) day.

so that's thats one modification for this.

this is the early session that i'm <unintelligible> of approach we need some domain adaptation again.

so i need to work on this which is so so this is a good place for better to use english as english models because i think that this english models could be better adapted.

also maybe [person1] can adapt the just the vocabulary of the [person5] talkage.

so the standard as that we are using.

but that would need input from people are not here.

from [person6] and maybe [person11].

so let's let's <unintelligible> they the optional.

so that is the early session for the adaptation.

and then the other domain adaptation session that will need that will be a harsh one.

so there will be then other monday seminars.

for these other monday seminars we don't have to do anything it's just an opportunity to try it again.

but one event will require that.

and that is there is two upcoming events so <other_yawn> actually event big for them maybe.

so the monday seminar on the 17th happens the week after we have some dry run of a workshop where we are describing various language technologies and that's for the [organization6] congress the part like the (site) activity.

we are not only subtitling but we are also advertising for nlp.

and already wednesday of the 12th of february we are having our first dry run.

and the idea is that that we say there will be also recorded and used for domain adaptation later on.

and the second dry run and then the final workshop.

so there will be us non-native speakers very likely presenting in english.

because the workshop will be held in english.

that would benefit from the same type of domain adaptation as we will need for the 17th of february.

so so there two events which have the same type of adaptation.

it's nlp and [project1] on the 12th and the 17th of february.

so two weeks from now and a little bit later.

and then the the important big event that i wanted to talk about is the students firm fair thats something which we have done only last year.

and last year nothing was really working.

and we were only reusing the system from [organization4] and it went for single minute of the two days.

but it was an important event for data collection because students are presenting their their companies and we record them and they transcribe it and they compete in how well their work- their voice was recognized.

and it's very noisy environment it's like fair or congress in big hall of many stands and there is little a little side it's not really room, it's just a dedicated area where this competition of the firm presentations is is running.

so it's a big challenge for equaliate.

and we are also going to (writers), well it will depend what the they will let us or not, last year we were we were showing some of the subtitles and because that was were appearing just once.

but because there is four hundred high school students, they are able to take pictures of that and they were then sharing that across networks they were laughing at at other teams.

so it is it is an risky activity.

and that's a big for us it's a big opportunity to test that.

so that's the 18th and till 20th march and for that we would highly benefit from having some profanity filtering.

so so this is an invitation.

this is one of the to do or wanted items to add some filter and f- populated what words that we don't want to show at all.

so it's it's not always only bad words it could be just words related to bad topics. <laugh>

so that's it is top so it's also the questions what to do of these topics then rather we should removes such words or not.

but this is something that we have a months or more for.

and we really should work on this.

some some censorship upcoding construct.

so as you remember on the the day before yesterday were [person3] giving (advering) talk one [location4] word which has nothing in relation that was translated as scrotum and so this is exactly the type of word which is bad for high school students.

so - <laugh>

so that's an invitation for for well domain adaptation because we don't know in part no no in part but kind of -

we may know the names of the companies at least and maybe some description live up what is their core bussiness or topic of business.

and to that we should adapt because they going to present their companies.

and it will be non-native speakers of english.

so this is already like a big challenge.

and then the filtering of the profanity would be would be <unintelligible> are useful for that.

so and i would like to ask everybody to to say what they did and think about what activity they would like to work on.

so maybe [person12] you can start.

(person7) okay.

this week i work on do the collection is business for [other1] and english.

and because the from the ger- [other1] thing of the office i have downloaded 1970 to 1980 pdf files which is extracted which was extracted from the [other2] using texted [other3] so just can work in text format.

and 15000 sentences is crawled automatic using automatic clover for english and [other1].

from these supreme article this.

so and we need i need clean all the data because some duplicate sentences and some other garbage values.

so (member) [person6] is i think he was busy exams all times.

(person2) who?

(person1) [person6].

(person2)  aha, okay.

(person7) sometime that united interrupted because -

so this is the picture.

(person2) ok, so so what would be your collaboration with [person6] then if you if you were available?

(person7) so we will -

we will use it -

(person2)he would use it as language model and he would extract the terms from that or -?

(person7) i don't know what he is using the tools what is the aplication so it would be nice when we would be -

(person2) so i didn't catch the details, the numbers.

maybe later if you can correct that.

but the first which is so far there were mostly in english or only in english or all the languages, that's a mix of languages?

(person7) <unintelligible> english and <unintelligible> [other1].

(person2) so only only english and [other1].

(person7) little bit. monolingualy.

(person2) so for the english or for both of them these are languages anyway so there the adaptation by [person6] make sense it's good to put this into the collection and start organizing the collection.

ok, great, so thanks.

(person1) so [person1] updated me yesterday.

there was some problem of the -

(person2) so i think i understood the problem from the comment.

so if the idea is that the compress sound goes always to network and is directly decompress before (begin sent).

so have you added the required command?

(person1) yes i could (interprate).

it's there.

(person2) so [person1] will test it whenever.

(person1) so that's one thing, second thing.

i was looking into that multisource model output <unintelligible> output [project2].

so i did some thing one that.

but one specific question i had was regarding the profanity filtery and give after [person3] <unintelligible>.

so let it circumstance we had that the that is not actually <unintelligible> that is not supposed to be a prophane what coming up.

(person2) <unintelligible>

(person1) yeah, yeah, so but i believe in the putting up the <unintelligible> top of list.

do you think i mean i don't think that briefly most effective thing because we would blocking on many  (?able).

so better something i would like to (aplain) like a like to thinkable.

(person2) so i have an idea but i don't know if we have the person for that and the idea is exactly responding to the to the fake that.

the list will never be complete and it will contain many word which we actually like would like to have an as the output or as the input.

so the idea is here the bad word was created by an error in the translation system.

so somehow there was lighter the [location4] word was "soupatko" <another language=""> which is a slider actually and for some reason in some of the corpora this must have been in the same sentence as as the error came from.

so it just made up a follow low frequency pattern which was unreliable.

so the idea i have in mind is to train empty systems on corpora which are refind to contain only higher frequency words so like safe word corpora or safe vocabulary corpora.

safe is not in the sense that it's not bad word but safe in the sense that it has enough occurancies.

and i would like to -

so this is if we do this type of filtering firstly together a very huge corpus then we need to set up threshold like what was the safe we can work work frequent boundary and then we need to create the corpus which is somehow limited to these words.

and that is hard, somewhat, but i think that we could do it monolingualy and use back translation to create the other part.

and maybe put out sentences which are <unintelligible>. 

there is two ways either we can brought sentences which contain infrequent word infrequent words or we can replace infrequent words in the sentences with some like placeholder things like forgetting.

so then we use the rest of the sentence and non just these kept these unknow words.

and the third option is to use this filtering on in the monolingual setting only and use backtranslation to get the other side.

and we need to evaluate which of these approaches is the best interupt.

so all preservation of quality and avoidance of bad words.

so this is like a large topic and it's totaly unlagged to refiltering on the (fly).

one further twist in the on the (fly) furthering would be like to add words live and we go.

so the tool that does this filtering should actually be regularly checking this has appearance some list.

so then we can immediately prevent <unintelligible> respond of them.

well in the tok on tuesday [person3] didn't say "soupatko" again so the scrotum did not reappear.

but still we would like to have a chance to prevent it from that.

so that's a like a bigger experiment and i would probably find someone new for that one semestr stars from the students but it's it is an option.

okay.

so yeah.

so your main homework word compression, the the further experiment in word multisource and just this comment on the limitation profanity filtering.

okay.

thank you.

so now maybe [person16].

(person16) so as you may know i finished evaluation framework and it's ready but maybe for some test maybe i should do some a small changes but i think it's ready.

we (approved) that time base and word base segmentation to find the for example where is the may i want to calculated delay i should estimate that time of each word should be the time expected time for each word.

so i used two different segmentation for this, one is timebase and second is wordbase anyway everything is almost ready. 

and now maybe i need something like <unintelligible> complete files to test it to test our there framework.

i prepare a small things myself manualy and it works now. 

so this is the team and now i me and [person8] working on the i and [person8] are working on the paper for (exceptement) and i think that's on.

so -

(person2) so the bad thing -

[person15] is on the call.

the bad thing is that i haven't heard from <unintelligible> -

a okay.

so there is four -

i've received that e-mail yesterday from[person11]

the mi corporas the mi corporas has time stamps for word.

so we should be able to convert that.

so the problematic setting.

[person15]?

are you on the call?

(person15)yes.

(person2) so the remember that we have the recorded from mi corporas where your forced alignment totaly fails.

and we have the idea of breaking in a small chunks.

so realize and then [person11] has double checked this corporas actually.

(person15) yeah i have already read a mail.

(person2) yeah, okay.

so i think that it would still be interesting to run your forced alignment of this data and we you can now use the existing time stamps to break the long recording into shorter ones.

so that you see whether it like cashes up with these shorter segments.

and that is important to know for some future corpora what will not have the time stamps.

(person15) okay.

i will looking to that.

(person2) but for for the evaluation we have now something that we can do <unintelligible> we will actually sometime.

it appears that thursday lunch time is the bad time.

last time it was also very noisy in this -.

(person16) maybe we should do it after lunch or maybe in the morning.

(person2) yes.

it 's i don't know i don't know why it's a case normaly recorders it's much quieters.

(person16) but the question <unintelligible> corpus we had the english and [location4] parts or only-

(person2) so no, we have only one recordings translated into [location4] and [other1].

(person16) okay, so it's a [other1] but we won't have do we have english is are we have - 

(person2) yes.

we have correct english transcript much large of volume and we have limited file only in [location4].

(person16) okay, and the (mode) english it's time (stamped).

(person2) yes.

(person16) okay.

(person2) this is what something which i didn't realise before because i <unintelligible> with that like intense enough myself.

(person16) yes.

so in this case i can convert the second we can use it but we can discuss about it later.

(person2) yeah, okay.

so to (cut) down, evaluation too is finished.

now, why do not -

<parallel_talk>

(person16) yes, i have a question about this (board) files.

is there any trajectory to how have together?

(person2) no we don't have that.

(person16) because unfortunately i'm not sure i have access to <unintelligible>.  

because i have access to some of them because it's a little bit - 

so maybe if we can have name of files. 

it could help someone somebody like me.

because i know we have lot of these files now.

(person1) <unintelligible> create a list of them and then send <unintelligible>.

(person16) maybe could help because i´m confused about this files i cannot find to use.

(person2) so that's <unintelligible>.

and now let's move to those who are present remotly.

so maybe [person5] if you can start?

you understand at all?

(person5) yes, yes.

i have been listening.

so i have mainly concentrated on the adaptation of czech asr for [person3] talk that took place yesterday.

so i've experimented with various technics and especially like domain adaptation of a language model and then acoustic fine tuning on [person3] talk.

so i think that i was quite happy with result then during the presentation as like quite a lot of domain specific words were recognized by the model.

so that's good sign for me that some how works these technics and -

yes, right now i'm working on a next version of the czech asr system that should be trained on even more data.

so i'm preparing this.

(person2) ok, so remember the thing that i said at the beginning.

please enter your achievements also into the google sheets so that thing once for that.

whenever you think it something big and i think this the improvement [person3] talk it's very good one so definitely deserve to be listed.  

please let's make sure you do that.

don't forget.

(person5) yeah, okay.

(person2) so now let's move to [person15].

(person15) so first question regarding the compression.

which kind of compression it is.

and it will be applied to all asr systems?

(person2) so that's something that we're just testing and [person1] said that he would evaluated.

[person1] has found the appropriate command flacs so that the audio is compress to mp3 then shiped as mp3 to network and then decompress before being sent to the asr.

and the question is what effect does this have on the asr quality.

so if the if there is something we can lost of of  <unintelligible>  then we don't want to do it but if it's like the same then will write to <unintelligible> because it simply makes the communication more (abased) less likely to be to be effected by network long.

so [person1] will evaluated soon and he can evaluated with everything and because had already been integrated.

and -

so so if your assistance -

i belive could evaluated yourself as well supportly or if your assistance will be already included and delivered stable output which is the problem that we still have.

then [person1] would evaluated for you.

(person15) yeah.

jus-

because i noticed when i read the paper about librispeech that the recordings were before in mp3 format and then they were actually converted into flac format and then i had to convert it into wav format so and the and the common voice is also in mp3.

so i guess that my systems are actually good with mp3 compression.

so that's the first thing had.

the second thing is i was working on the sound segmentation.

it's there is a problem with asr system because some words get cut in the middle.

now my systemlooks in the window with a greedy decoding and looks for pauses bef- between words and i have window that must be at least four seconds long and maximum is eight seconds found find the the most probable pause between the words and then i cut the windows there.

so this makes the decoding a bit better.

i also trained the transformer converting the phonings into graphians and i tested to the settings.

but the results were at least for at least for some [organization2] talks they were really bad because the the corpora on which the transform were trade was trained is for casual speak and and fairy tales and so on.

so that's actual translation with the with the transformer was quite bad and the transform-hallucinated some words.

it was funny (laugh).

and i i sent an e-mail to [person18] and i haven't received any any comments to to this new segment- to this new windowing from him.

so -

(person2) sorry.

so you have already integrated right?

so anybody can tests it at this news.

(person15) no, i had some temporary folder for this hot updates but i need some feedback from [person18] of how this works with his segmenter.

and i will have some more recording from the from yesterday so we can use use this non-native english for some find uniquals our models.

(person2) okay.

so do you estimate that than your segmenter could be operational in the two weeks from now for the monday talk by 17th or even for the dry run sesion of the workshop on the 12th and you can talk.

(person15) i hope so but i haven't heard from [person18] yet.

so i don't know wheter it works.

and the next problem is that the segmentation should be done on speaker separately because when there is some conversation the windows can overlap to speakers and the problem is then then the transformer translate these sentences into nonsense.

(person2) the problem is that we don't have speaker diarization on (fly) so we simply do not know when there is a speaker change.

(person5) yeah that's a problem but if if if the talks are only with one speaker then there is no problem with this.

(person2) so the talks in general are one speaker only but do it also as an for example for the remote calls there conference calls on the interview platform, there each speaker has different chanel so the diarization is there for free and it´s not not mixed.

(person15) that sould be better and the next problem is that the windows overlaped sentences and this causes some problems too.

and i i now i don't have some- now no solution to this problem.

(person2) yeah.

so so are you waiting for [person18] also please getting (dash) with [person1] because i think that [person1] made be more responsive these days. 

[person1] is coming back to back to [location1] on the weekend.

who won't be available during the <unintelligible>

then may have more time for for that.

(person15) good.

i will have new better models by [person11] so maybe this will work.

(person2) yeah, okay.

thank you.

so everybody please review what have in document i was trying to do something but i'm not perfect i can roll it follow.

so the last person we have here today is [person14].

so i don't know if the rest of team has ever met you in person.

maybe not.

[person14] has already worked on the project last june when he was helping us to put all the (byplan) together and -

so i just remember that his reimplementation or his treak of the tee programme tee rescue the project because tee was cashing its output.

and we need the (byplans) to be wilow cashing so we created an modify version and there was there was one of the early sessions. 

and [person14] is now in [location3].

so he can participate only remotedly and there for also whatever he does has to be like self contain as little interaction with others as as possible.

so now [person14] please let us know what if you have work on anything.

i think you work on paraphrasing.

so you are in touch with some other colleagues from here who are actually <unintelligible> today.

but in the paraphrasing goal it's not for [project1] that's for different that's for different project.

so is there anything that you have work on it's useful for [project1] from the subtitling.

(person10) yes, so so far i have only worked on the paraphrasing and actually i just w- i just wanted to say that right now i'm just waiting for a virtual machine for the paraphrasing server but otherwise it is done and working.

otherwise i was planning to work on to work on the division <unintelligible> of of the levels of of microphone. 

i know is that still -

(person2) it is still deleted.

(person9) okay, if it is still deleted then i can then i can started next week.

but my first priority was the paraphrasi- was the paraphrasing so i didn't didn't <unintelligible> yet.

(person2) so that the problem is that [person18] is not responding.

i remember that i have asked him to check if the virtual machine set up is is reasonable and something like that and then forward to the it department.

if you don't get any response from [person18] even after the weekend then please make sure to to like treasure ask for that yourself without waiting.

(person10) okay, okay so i don't have anything interesting to report you right now.

(person2) yeah.

the visualisation of sound it's something that we will need.

this is something that [person1] would check.

i think it should be simply and totaly independent process for now which anybody could run to adition to be sound acqusition pipeline and would that see which of the input chanell is receiving what output.

so it's very similar to audacity recording chanells except it is not recording.

(person10) yeah, okay.

ou, there is one thing that isn't exactly i don't know important maybe.

but then i just saw it today.

and i told you might be interested in it. 

i saw that there's any paper from [organization3] from december on asr and it is called fastspeech.

have you have you seen it?

(person2) i heard about it somewhere where have i saw it -

(person10) yeah, so i just try it today and it's just basicly sais that it´s like three hundred times faster for asr so so so it could i just told it could actually be used to do asr like on on the spot and it couldn't have to be sent and the delay problem could be eliminated.

(person2) ehmm

(person10) and they also say it is that it is robust and dust nearly eliminates words keeping <unintelligible> which is also interesting interesting so i can just send it to you if you like.

(person2) well the question is wheter it is available as (code), it is? is it?

(person10) i haven't i haven't get didn't get (try) yet. <laugh>

(person2) so that in that case that would be obviously interesting so that we would try in our set up.

(person10) yeah, i <unintelligible>.

(person2) fastspeech <parallel_talk>, yeah, it seems like it it should <unintelligible>.

so so again.

i think i wanted <unintelligible>.

<parallel_talk>

check if there is code for that and integrate - <parallel_talk>

so that's that's it.

thank you.

(person10) anyway i do my exams period is ending maybe i would be able to come to [location1] for for maybe one week or something like that.

(person2) yeah, okay.

and a colleague of mine ask me wheter you are from [location5] or not and i don't remember.

(person10) no no.

(person2) are you? are you from kos-

(person10) no, i'm not, i'm from [location2].

(person2) [location2] okay.

yeah, so that's different. <laugh>

so, i think that's we should stop again today it's then spent more time we wanted.

please review this list and please think what you to do from the wanted topics.

so that next week we should be meeting at the same time, still thursday and -

okay, so what so you prefer friday instead of thursday right?

so should we all then make do a call for a doodle because thing changed for others as well.

<parallel_talk>

okay so.

(person5) bye.

thank you much.

(person10) bye.

(person15) bye.
